{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Notation\n",
    "\n",
    "- Training set: $ \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\;..., (x^{(m)}, y^{(m)})  \\} $\n",
    "\n",
    "- No. layers in the network: $ L $\n",
    "- No. units in layer $l$ : $ s_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification ($y = 0$ or $1$)\n",
    "\n",
    "Single output unit.\n",
    "\n",
    "$h_\\Theta (x) \\in \\mathbb{R} $\n",
    "\n",
    "$s_L = 1 \\;\\;;\\;\\;  K = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification (K classes)\n",
    "\n",
    "$ y \\in \\mathbf{R}^K $\n",
    "\n",
    "$K$ output units\n",
    "\n",
    "$h_\\Theta (x) \\in \\mathbb{R}^K $\n",
    "\n",
    "$s_L = K \\;\\;;\\;\\;  K = \\geqslant 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalisation of logistic regression cost function.\n",
    "\n",
    "__Logistic regression cost function__\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\left[\\sum\\limits_{i=1}^m y^{(i)}\\;log h_\\theta(x^{(i)}) + (1-y^{(i)})log(1 - h_\\theta(x^{(i)}) \\right] + \\frac{\\lambda}{2m}\\sum\\limits_{j=1}^n\\theta^2_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Neural network cost function__\n",
    "\n",
    "If $h_\\Theta (x) \\in \\mathbf{R}^K ,\\;\\; (h_\\Theta (x))_i = i^{th}$ output of the neural network.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(\\Theta) = -\\frac{1}{m} \\left[\\sum\\limits_{i=1}^m \\sum\\limits_{k=1}^K y^{(i)}_K\\;log( h_\\theta(x^{(i)}) )_K + (1-y^{(i)}_K)\\;log(1 - h_\\theta(x^{(i)})_K \\right]\\\\\n",
    "+ \\frac{\\lambda}{2m}\\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_l+1} (\\theta^{(l)}_{ji})^2 \n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing $h_\\Theta()_K$, the $K^{th}$ of of the nn, with the respective y value, $y_K$. For all training exampples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relgularization term, $(\\Theta^{(l)}_{ji})^2$, is computed for every $j, i , l$ (# neurons, # layers,  )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN backpropagation \n",
    "\n",
    "__Gradient computation__\n",
    "\n",
    "$J(\\Theta)$ as defined above\n",
    "\n",
    "Goal is to minimise $J(\\Theta)$: $ \\min\\limits_\\Theta J(\\Theta) $\n",
    "\n",
    "In order to do so, one must compute:\n",
    "\n",
    "   - $J(\\Theta)$\n",
    "   - $\\frac{\\partial}{\\partial \\Theta^{(l)}_{ij} } J(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Forward propagation:__\n",
    "\n",
    "On one training example\n",
    "\n",
    "(assuming 4 layers)\n",
    "\n",
    "$ a^{(1)} = x $\n",
    "\n",
    "$ z^{(2)} = \\Theta^{(1)}a^{(1)} $\n",
    "\n",
    "$ a^{(2)} = g(z^{(2)}) $\n",
    "\n",
    "$ z^{(3)} = \\Theta^{(2)}a^{(2)} $\n",
    "\n",
    "$ a^{(3)} = g(z^{(3)}) $\n",
    "\n",
    "$ z^{(4)} = \\Theta^{(3)}a^{(3)} $\n",
    "\n",
    "$ a^{(4)} = h_\\Theta(x) = g(z^{(4)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backward propagation:__\n",
    "\n",
    "On one training example\n",
    "\n",
    "(assuming 4 layers)\n",
    "\n",
    "For each node $\\delta^{(l)}_j = $ 'error' of node $j$ in layer $l$. Error of the activation unit ($a^{(l)}_j$ activation unit of unit $j$ in layer $l$)\n",
    "\n",
    "For each output unit (layer $L=4$)\n",
    "\n",
    "$\\delta^{(4)}_j = a^{(4)}_j - y_j = (h_\\Theta(x) )_j - y_j $\n",
    "\n",
    "$\\bar{\\delta^{(4)}} = \\bar{a^{(4)} } - \\bar{y}$\n",
    "\n",
    "For the two iternal layers:\n",
    "\n",
    "$\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)} .* g'(z^{(3)}) $\n",
    "\n",
    "Where $g'(z^{(3)})$ can be simplified to $ a^{(3)} .* (1-a^{(3)} $\n",
    "\n",
    "$\\delta^{(2)} = (\\Theta^{(2)})^T\\delta^{(3)} .* g'(z^{(2)}) $ and $ a^{(2)} .* (1-a^{(2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Algorithm__\n",
    "\n",
    "Training set $ \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\;..., (x^{(m)}, y^{(m)})  \\} $\n",
    "\n",
    "Set $\\Delta^{(l)}_{ij} = 0$ (for all $l, i, j$)\n",
    "\n",
    "For $i$ = 1 to $m$\n",
    "\n",
    "$\\;\\;\\;\\;$ Set $a^{(1)} = x^{(1)}$\n",
    "\n",
    "$\\;\\;\\;\\;$ Forward propagation to compute $a^{(l)}$ for $l = 2, 3,\\;..., L$\n",
    "\n",
    "$\\;\\;\\;\\;$ Compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
    "\n",
    "$\\;\\;\\;\\;$ Compute $\\delta^{(L-1)}, \\delta^{(L-2)},\\;..., \\delta^{(2)}$\n",
    "\n",
    "$\\;\\;\\;\\;$ $\\Delta^{(l)}_{ij} := \\Delta^{(l)}_{ij} + a^{(l)}_j\\delta^{(l+1)}_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($\\Delta^{(l)}_{ij} := \\Delta^{(l)}_{ij} + a^{(l)}_j\\delta^{(l+1)}_i$ can be vectorized as $\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside the For loop:\n",
    "\n",
    "$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ if $j = 0$\n",
    "\n",
    "$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\lambda\\Theta^{(l)}_{ij} \\;$ if $j \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $D$ is equivalent to calculating the derivative terms:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial \\Theta^{(l)}_{ij}} J(\\Theta) = D^{(l)}_{ij} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Unrolling Parameters\n",
    "\n",
    "Optimisation routines take gradient & cost. They arrume both variables are vectors.\n",
    "\n",
    "However it is not the case in our neural network. Where both the gradients and the cost are both matrices. They both need to be __unrolled__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "Assume a neural network $s_1 = 10, s_2 = 10, s_3 = 1$\n",
    "\n",
    "The weghts and gradient matrices are of sizes:\n",
    "\n",
    "$ \\Theta^{(1)} \\in \\mathbb{R}^{10\\times11},  \\Theta^{(2)} \\in \\mathbb{R}^{10\\times11}, \\Theta^{(3)} \\in \\mathbb{R}^{1\\times11} $ \n",
    "\n",
    "$ D^{(1)} \\in \\mathbb{R}^{10\\times11}, D^{(2)} \\in \\mathbb{R}^{10\\times11}, D^{(3)} \\in \\mathbb{R}^{1\\times11} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn them into vectors they will need to be unrolled. Once the optimisation has completed. After that they will be reshaped into their original sizes. \n",
    "\n",
    "This can be done in Octave as follows:\n",
    "\n",
    "```Octave\n",
    "\n",
    "thetaVec = [Theta1(:); Theta2(:); Theta3(:)];\n",
    "DVec = [D1(:); D2(:); D3(:)];\n",
    "\n",
    "Theta1 = reshape(thetaVec(1:110), 10, 11);\n",
    "Theta2 = reshape(thetaVec(111:220), 10, 11);\n",
    "Theta3 = reshape(thetaVec(221:231), 1, 11);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0001000000000055"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1+0.01)**3-(1-0.01)**3)/(2*0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
