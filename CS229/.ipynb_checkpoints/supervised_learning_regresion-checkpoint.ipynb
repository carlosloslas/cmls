{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single variable linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notation:__\n",
    "* m $\\rightarrow$ number of training examples\n",
    "* x $\\rightarrow$ features\n",
    "* y $\\rightarrow$ target\n",
    "* (x,y) $\\rightarrow$ one training example\n",
    "* (x$^{(i)}$,y$^{(i)}$) $\\rightarrow$ i$^{th}$ training example ($i$ is an index into the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING SET $\\rightarrow$ LEARNING ALGORITHM $\\rightarrow$ HYPOTHESIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x $\\rightarrow$ HYPOTHESIS $\\rightarrow$ estimated value of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis__: $h_\\theta(x) = \\theta_0 + \\theta_1x$ (linear function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above is named as __Univariate linear regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notation:__\n",
    "* $\\theta$ $\\rightarrow$ parameters\n",
    "\n",
    "The cost fucntion enables us to obtain the values of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $\\theta_0$ and $\\theta_1$ so that $h_\\theta(0)$ is close to y for our training examples (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $\\theta_0$ and $\\theta_1$ so that the average, the 1 over the 2m, times the sum of square errors between my predictions on the training set minus the actual values of the houses on the training set is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimise the average ($ \\frac{1}{m}$) square of the error between the predicted and the actual value of $y$. For easier math $ \\frac{1}{2}$\n",
    "\n",
    "Minimise $\\theta_0$ and $\\theta_1$ \n",
    "$ \\frac{1}{2m}\\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) - y^{(i)} )^2$ (m = # of training examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cost function__: \n",
    "\n",
    "$$ J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) -  y^{(i)} )^2 $$\n",
    "\n",
    "Also called __square error cost fucntion__ (most common cost function)\n",
    "\n",
    "Obtain $\\theta_0$, $\\theta_1$, such that $J$ is minimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost fucntion intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section intended to give a better inderstanding of $h_\\theta(x)$ and $J(\\theta)$. Methodology used:\n",
    "1. Reduce the hypothesis to $h_\\theta(x) = \\theta_1x$\n",
    "2. Manually calculate h and J for a range of problems\n",
    "3. Repeat for the hypothesis $h_\\theta(x) = \\theta_0 + \\theta_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Some function: $J(\\theta_0, \\theta_1)$\n",
    "\n",
    "Requirement: $min_{\\theta_0, \\theta_1}$ $J(\\theta_0, \\theta_1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "repeat until convergence{\n",
    "\n",
    "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1) $$\n",
    "(for $j=0$ and $j=1$)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\alpha$ is the learning rate\n",
    "\n",
    "This is used to __simultaniously update__ $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "temp0 = $:= \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_0}J(\\theta_0, \\theta_1)$\n",
    "\n",
    "temp1 = $:= \\theta_0 - \\alpha \\frac{\\partial}{\\partial \\theta_1}J(\\theta_0, \\theta_1)$\n",
    "\n",
    "$\\theta_0$:= temp0\n",
    "\n",
    "$\\theta_1$:= temp1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the gradient of J is positive the updated term will be smaller. Contrary, if the slope of J is negative, the updated term will be larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of slow convergence ($\\alpha$ too small) and overshooting ($\\alpha$ too large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(outside note)\n",
    "Evenly scaling of features contributes to equate the effect of each of the features used in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the partial terms which are used to update $\\theta_0$ and $\\theta_1$, the following is obtained:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1) = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum\\limits^m_{i=1} (h_\\theta(x^{(i)}) -  y^{(i)} )^2 $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0}J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum\\limits^m_{i=1} ( h_\\theta(x^{(i)}) - y^{(i)} ) $$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1}J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum\\limits^m_{i=1} ( h_\\theta(x^{(i)}) - y^{(i)} ) x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm can be updated:\n",
    "\n",
    "repeat until convergence{\n",
    "\n",
    "temp0 = $:= \\theta_0 - \\frac{1}{m}\\sum\\limits^m_{i=1} ( h_\\theta(x^{(i)}) - y^{(i)} )$\n",
    "\n",
    "temp1 = $:= \\theta_1 - \\alpha \\frac{1}{m}\\sum\\limits^m_{i=1} ( h_\\theta(x^{(i)}) - y^{(i)} ) x^{(i)}$\n",
    "\n",
    "$\\theta_0$:= temp0\n",
    "\n",
    "$\\theta_1$:= temp1\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent the cost fucntion has no local optima since it is a __convex function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\"Batch\" Gradient Descent__\n",
    "\n",
    "Each step of gradient descent uses all the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi variable linear regression\n",
    "\n",
    "A larger number of features allows to better make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation:\n",
    "* $x_1$, $x_2$, ..., $x_n$ $\\rightarrow$  Features\n",
    "* $y$ $\\rightarrow$ Variable to predict\n",
    "* $n$ $\\rightarrow$ Number of features\n",
    "* $m$ $\\rightarrow$ Number of samples\n",
    "* $x^{(i)}_j$ $\\rightarrow$ Value of feature $j$ of $i^{th}$ training example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "$$h_{\\theta}(x) =  (\\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n)$$\n",
    "\n",
    "This can be thought of as if $\\theta_j$ is the effect of feature $j$ in the predicted variable $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, a $x_0$ term is defined: $$ x_0  = 1 \\rightarrow x^i_0 = 1$$ for all training samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature and parameter vectors will be:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots\\\\\n",
    "    x_{n} \n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}\\;\\; ; \\;\\;\n",
    "\\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\theta_{1} \\\\\n",
    "    \\theta_{2} \\\\\n",
    "    \\vdots\\\\\n",
    "    \\theta_{n} \n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the hypothesis can be reformulated as:\n",
    "\n",
    "$$h_{\\theta}(x) = \\sum^n_0 (\\theta_0x_0 + \\theta_1x_1 + ... + \\theta_nx_n)$$\n",
    "\n",
    "Since $x_0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be rewritten in matrix notation as:\n",
    "\n",
    "$$h_{\\theta}(x) = \\underline{\\theta}^T \\underline{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this can also be referec to as __Multi variate linear regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothesis__: $h_{\\theta}(x) = \\underline{\\theta}^T \\underline{x}$\n",
    "\n",
    "__Cost function__: $J(\\vec{\\theta}) =\\frac{1}{2m} \\sum\\limits^{m}_{i=1}(h_\\theta(x^{(i)}) - y^{(i)} )^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Algorithm__:\n",
    "\n",
    "Repeat until convergence{\n",
    "\n",
    "$ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum\\limits^m_{i=1}(h_\\theta(x^{(i)}) - y^{(i)} )x^{(i)}_j $\n",
    "\n",
    "(simultaniously update for j=0, 1, ..., n)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "Make sure features are on a similar scale.\n",
    "\n",
    "## Mean normalization\n",
    "\n",
    "Replace $x_i \\rightarrow (x_i - \\mu_i)$ to make features have approximately zero mean\n",
    "\n",
    "$$ x_i = \\frac{x_i - \\mu_i}{\\sigma_i} $$\n",
    "\n",
    "where $\\mu_i$ is the mean and $\\sigma_i$ is the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence of linear descent\n",
    "\n",
    "The value of each of the features should converge as the number of interations increases. \n",
    "\n",
    "Furthermore the cost function should also decrease with the number of iterations.\n",
    "\n",
    "(personal note) Can be automated by checking the gradient of the cost fucntion with respect to the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing $\\alpha$\n",
    "\n",
    "Use some scale:\n",
    "\n",
    "0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features & Polynomial regrassion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be useful to create a new feature to aid the linear regresion.\n",
    "\n",
    "Again, it may be usefull to modify the model (linear, quadratic, exponential, ...)  to better fit the data. With higher order terms, feature scaling becomes key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation\n",
    "\n",
    "Solve $\\vec{\\theta}$ for a minimum in $J(\\vec{\\theta})$ analytically. Where $\\vec{\\theta} \\in \\mathbb{R}^{n+1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Cost function__: $ J(\\theta_0, \\theta_1, ..., \\theta_m) = \\frac{1}{2m} \\sum\\limits^m_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We define the design matrix $\\underline{X}$ and the vector $\\vec{y}$. $\\underline{X}$ has a size of $m \\times (n+1)$ (# of training examples $\\times$ # of features + $x_0$). $\\vec{y}$ is an $m$ dimensional vector (# of training examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of $m$ training examples $(x^{(1)}, y^{(1)})$, ..., $(x^{(m)}, y^{(m)})$\n",
    "\n",
    "And $n$ features $ x^{(i)} = \n",
    "\\begin{bmatrix}\n",
    "    x^{(i)}_{0} \\\\\n",
    "    x^{(i)}_{1} \\\\\n",
    "    x^{(i)}_{2} \\\\\n",
    "    \\vdots\\\\\n",
    "    x^{(i)}_{n} \n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1} $\n",
    "\n",
    "The design matrix $\\underline{X}$ and the vector $\\vec{y}$ would be:\n",
    "\n",
    "$$\n",
    "\\underline{X} = \n",
    "\\begin{bmatrix}\n",
    "    x^{(1)}_{0} x^{(1)}_{1} x^{(1)}_{2} \\dots  x^{(1)}_{n}\\\\\n",
    "    (x^{(i)}_{2})^T \\\\\n",
    "    \\vdots\\\\\n",
    "    (x^{(i)}_{m})^T\n",
    "\\end{bmatrix} \n",
    "\\;\\; ; \\;\\;\n",
    "\\vec{y} = \n",
    "\\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    y^{(2)} \\\\\n",
    "    y^{(3)} \\\\\n",
    "    \\vdots\\\\\n",
    "    y^{(m)} \n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By calculating $ \\vec{\\theta} = (\\underline{X}^{T} \\underline{X})^{-1} \\underline{X}^{T} \\vec{y}$ the value of $\\vec{\\theta}$ which minimises the cost fucntion is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
