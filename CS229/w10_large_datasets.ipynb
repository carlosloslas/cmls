{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation of 100,000,000 examples is very computational expensive. \n",
    "\n",
    "__Sanity Check!__: Try first on a small training set & verify that the algorithm has a __high variance__ (plot on the left) when m is small. Since the problem of high variance can be solved by using a larger training dataset, using the full 100,000,000 example dataset will result in a well trained algorithm. \n",
    "\n",
    "If found on the __high bias__ problem, increasing the number of features, or adding the number of hidden units in the Neural Network will help to solve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use linear regression as an example, however SGD can be applied to any learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_sgd_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient term is very computationally expensive to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_sgd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_sgd_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_sgd_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can work faster than stochastic gradient descent. Use a small batch of the data and carry out gradient descent on that. Loop through all the batches in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_mbgd_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_mbgd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MBGD will outperform SGD if the implementation is well vectorized. This will loop through the dataset quicker and achieve the results faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b usually equal to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence of stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for convergence in batch gradient descent & stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_csgd_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost decreases & platoes over time. Smaller learning rate is slower initially, however the final cost can be lower. (top left)\n",
    "\n",
    "Increasing the number of examples averaging over produces a smoother curve, however the cost would be calculated less frequently. (top right)\n",
    "\n",
    "Flat oscilating curves could mean that the algorithm isnt learning much. However averaging over a larger number of samples could help show the smoother curve. If curve is still flat when increasing the number of points it averages, the learning algorithm is not learning properly. Decreasing the learning rate or changing the features could help solve this problem. (bottom left)\n",
    "\n",
    "If learning rate is increasing the algorithm is diverging and the learning rate should be reduced. (bottom right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_csgd_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters will not converge to the global rate. It will hover around it as shown in the figure.\n",
    "\n",
    "A way to solve this is to reduce the learning rate as the iterations increase. Not very common since learning rate has the added constants which need to be tunned. However if achieved the results will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_csgd_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_csgd_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from a continuous stream of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_ol_1.png)\n",
    "\n",
    "![title](pictures/ld_ol_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map reduce & Data parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale learning algorithms to higher sizes than stochastic learning descent. Applicable to m = 400,000,000 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Split the dataset into the number of available machines. Batch gradient descent for the portions of the data carried out in the available machines. Parameters updated based on the joined portions of the calculated batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_mr_1.png)\n",
    "![title](pictures/ld_mr_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to the map reduce is weather the learning rate & gradients can be expressed as the sum over the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_mr_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map reduce can also be parallelised over multi-core machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pictures/ld_mr_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
